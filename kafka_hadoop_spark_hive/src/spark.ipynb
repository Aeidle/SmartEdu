{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import BinaryType\n",
    "\n",
    "# from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize SparkSession\n",
    "# spark = SparkSession.builder.appName(\"ReadVideoFromHDFS\").getOrCreate()\n",
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-14 15:40:36.685394: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-14 15:40:36.738979: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-14 15:40:36.739018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-14 15:40:36.740500: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-14 15:40:36.748849: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-14 15:40:36.749806: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-14 15:40:38.198239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/14 15:40:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 136\u001b[0m\n\u001b[1;32m    132\u001b[0m video_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hdfs_video_folder, file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(hdfs_video_folder) \u001b[38;5;28;01mif\u001b[39;00m\n\u001b[1;32m    133\u001b[0m                file\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.avi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mkv\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Distribute video paths using Spark\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m videos_rdd \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumSlices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvideo_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Process videos in parallel using dowork function\u001b[39;00m\n\u001b[1;32m    139\u001b[0m results_rdd \u001b[38;5;241m=\u001b[39m videos_rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x : dowork(x))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py:812\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(c):\n\u001b[1;32m    810\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(c)  \u001b[38;5;66;03m# Make it a list so we can compute its length\u001b[39;00m\n\u001b[1;32m    811\u001b[0m batchSize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumSlices\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batchSize \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1024\u001b[39m)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    813\u001b[0m )\n\u001b[1;32m    814\u001b[0m serializer \u001b[38;5;241m=\u001b[39m BatchedSerializer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbatched_serializer, batchSize)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreader_func\u001b[39m(temp_filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from collections import Counter\n",
    "from deepface import DeepFace\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "\n",
    "# Step 1: Split video to frames with OpenCV\n",
    "def split_video_to_frames(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    framerate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frames = []\n",
    "    selected_frames = []\n",
    "\n",
    "    # Calcul du nombre total de frames dans la vidéo\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calcule l'intervalle entre chaque image à extraire\n",
    "    intervalle = total_frames // 6\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    for i in range(6):\n",
    "        # Définit le numéro de frame à extraire\n",
    "        frame_num = intervalle * i\n",
    "        selected_frames.append(frames[frame_num])\n",
    "\n",
    "    cap.release()\n",
    "    return frames,selected_frames, framerate\n",
    "\n",
    "\n",
    "# in this fucntion all the work is done here so that in spark we launch only dowork()\n",
    "\n",
    "def dowork(x):\n",
    "    # Step 2: Take x frames as a list and select a frame as the original\n",
    "    # video_path = \"classrom1.mp4\"\n",
    "    video_path = x\n",
    "    all_frames,selected_frames, original_frame_rate = split_video_to_frames(video_path)\n",
    "    # selected_frames = all_frames[:10]  # Replace x with the desired number\n",
    "    original_frame = selected_frames[0]\n",
    "\n",
    "    # Step 3: Original frame with histogram equalization\n",
    "\n",
    "    # Convert the image to YUV color space\n",
    "    yuv_image = cv2.cvtColor(original_frame, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    # Apply histogram equalization to the Y channel (luminance)\n",
    "    yuv_image[:, :, 0] = cv2.equalizeHist(yuv_image[:, :, 0])\n",
    "\n",
    "    # Convert the image back to BGR color space\n",
    "    original_frame_equalized = cv2.cvtColor(yuv_image, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "    # Step 4: Original frame with Bilateral filter (adjust the parameters if needed)\n",
    "    original_frame_denoised = cv2.bilateralFilter(original_frame, 9, 75, 75)\n",
    "\n",
    "    # Step 5: Original frame with Canny filter\n",
    "    original_frame_canny = cv2.Canny(cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY), 50, 150)\n",
    "\n",
    "    # Step 6: Use DeepFace for face and age detection\n",
    "    # Step 7: Generate a new video with face and age detection\n",
    "    output_video_path = \"output_video.mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, original_frame_rate,\n",
    "                          (original_frame.shape[1], original_frame.shape[0]))\n",
    "    # Load pre-trained face detection model\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Initialize a counter for emotion frequencies\n",
    "    emotion_counter = Counter()\n",
    "\n",
    "    # Assuming 'all_frames' is a list of frames obtained from a video\n",
    "    for frame in all_frames:\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.03, minNeighbors=5, minSize=(10, 10))\n",
    "\n",
    "        # Process each detected face\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract the face region\n",
    "            face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "            # Analyze the face for age and emotion\n",
    "            result = DeepFace.analyze(face_roi, actions=['age', 'emotion'], enforce_detection=False)\n",
    "\n",
    "            # Access age and emotion information for each face\n",
    "            age = result[0]['age']  # Access the first face in the list\n",
    "            emotion = result[0]['dominant_emotion']  # Access the first face in the list\n",
    "\n",
    "            # Update the emotion counter\n",
    "            emotion_counter[emotion] += 1\n",
    "\n",
    "            # Draw bounding box around the face with age and emotion information\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Age: {age}, Emotion: {emotion}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                        (0, 255, 0),\n",
    "                        2)\n",
    "\n",
    "        # Save the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "    cv2.imwrite(\"original_frame_equalized.jpg\", original_frame_equalized)\n",
    "    cv2.imwrite(\"original_frame_denoised.jpg\", original_frame_denoised)\n",
    "    cv2.imwrite(\"original_frame_canny.jpg\", original_frame_canny)\n",
    "    cv2.imwrite(\"originale.jpg\", original_frame)\n",
    "\n",
    "    # Print the emotion frequencies\n",
    "    print(\"Emotion Frequencies:\")\n",
    "    for emotion, count in emotion_counter.items():\n",
    "        print(f\"{emotion}: {count}\")\n",
    "\n",
    "    return \"Emotion Frequencies:\", emotion_counter.items()\n",
    "\n",
    "# creating spark application :\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"FaceDetectionApp\").getOrCreate()\n",
    "\n",
    "# Specify the HDFS folder containing video files\n",
    "# hdfs_video_folder = \"hdfs://your-hdfs-path/to/your/video/folder\"\n",
    "hdfs_video_folder = \"./\"\n",
    "\n",
    "# Get a list of all video files in the HDFS folder\n",
    "video_paths = [os.path.join(hdfs_video_folder, file) for file in os.listdir(hdfs_video_folder) if\n",
    "               file.lower().endswith(('.mp4', '.avi', '.mkv'))]\n",
    "\n",
    "# Distribute video paths using Spark\n",
    "videos_rdd = spark.sparkContext.parallelize(video_paths, numSlices=len(video_paths) * 3)\n",
    "\n",
    "# Process videos in parallel using dowork function\n",
    "results_rdd = videos_rdd.map(lambda x : dowork(x))\n",
    "\n",
    "# Collect the results\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n",
    "\n",
    "# Process the collected results as needed\n",
    "for result in results:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
