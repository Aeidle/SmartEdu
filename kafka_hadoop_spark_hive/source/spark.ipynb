{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from collections import Counter\n",
    "from deepface import DeepFace\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import base64\n",
    "\n",
    "\n",
    "# Step 1: Split video to frames with OpenCV\n",
    "def split_video_to_frames(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    framerate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frames = []\n",
    "    selected_frames = []\n",
    "\n",
    "    # Calcul du nombre total de frames dans la vidéo\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calcule l'intervalle entre chaque image à extraire\n",
    "    intervalle = total_frames // 6\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    for i in range(6):\n",
    "        # Définit le numéro de frame à extraire\n",
    "        frame_num = intervalle * i\n",
    "        selected_frames.append(frames[frame_num])\n",
    "\n",
    "    cap.release()\n",
    "    return frames,selected_frames, framerate\n",
    "\n",
    "\n",
    "# in this fucntion all the work is done here so that in spark we launch only dowork()\n",
    "\n",
    "def dowork(x):\n",
    "    # Step 2: Take x frames as a list and select a frame as the original\n",
    "    # video_path = \"classrom1.mp4\"\n",
    "    video_path = x\n",
    "    pp = video_paths[0].split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "    all_frames,selected_frames, original_frame_rate = split_video_to_frames(video_path)\n",
    "    # selected_frames = all_frames[:10]  # Replace x with the desired number\n",
    "    original_frame = selected_frames[0]\n",
    "\n",
    "    # Step 3: Original frame with histogram equalization\n",
    "\n",
    "    # Convert the image to YUV color space\n",
    "    yuv_image = cv2.cvtColor(original_frame, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    # Apply histogram equalization to the Y channel (luminance)\n",
    "    yuv_image[:, :, 0] = cv2.equalizeHist(yuv_image[:, :, 0])\n",
    "\n",
    "    # Convert the image back to BGR color space\n",
    "    original_frame_equalized = cv2.cvtColor(yuv_image, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "    # Step 4: Original frame with Bilateral filter (adjust the parameters if needed)\n",
    "    original_frame_denoised = cv2.bilateralFilter(original_frame, 9, 75, 75)\n",
    "\n",
    "    # Step 5: Original frame with Canny filter\n",
    "    original_frame_canny = cv2.Canny(cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY), 50, 150)\n",
    "\n",
    "    # Step 6: Use DeepFace for face and age detection\n",
    "    # Step 7: Generate a new video with face and age detection\n",
    "    output_video_path = f\"/SmartEdu_src/result/output_video___{pp}.mp4\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, original_frame_rate,\n",
    "                          (original_frame.shape[1], original_frame.shape[0]))\n",
    "    # Load pre-trained face detection model\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Initialize a counter for emotion frequencies\n",
    "    emotion_counter = Counter()\n",
    "\n",
    "    # Assuming 'all_frames' is a list of frames obtained from a video\n",
    "    for frame in all_frames:\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Process each detected face\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract the face region\n",
    "            face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "            # Analyze the face for age and emotion\n",
    "            result = DeepFace.analyze(face_roi, actions=['age', 'emotion'], enforce_detection=False)\n",
    "\n",
    "            # Access age and emotion information for each face\n",
    "            age = result[0]['age']  # Access the first face in the list\n",
    "            emotion = result[0]['dominant_emotion']  # Access the first face in the list\n",
    "\n",
    "            # Update the emotion counter\n",
    "            emotion_counter[emotion] += 1\n",
    "\n",
    "            # Draw bounding box around the face with age and emotion information\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Age: {age}, Emotion: {emotion}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                        (0, 255, 0),\n",
    "                        2)\n",
    "\n",
    "        # Save the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "    cv2.imwrite(f\"/SmartEdu_src/result/original_frame_equalized___{pp}.jpg\", original_frame_equalized)\n",
    "    cv2.imwrite(f\"/SmartEdu_src/result/original_frame_denoised___{pp}.jpg\", original_frame_denoised)\n",
    "    cv2.imwrite(f\"/SmartEdu_src/result/original_frame_canny___{pp}.jpg\", original_frame_canny)\n",
    "    cv2.imwrite(f\"/SmartEdu_src/result/original_frame___{pp}.jpg\", original_frame)\n",
    "\n",
    "\n",
    "\n",
    "    # Print the emotion frequencies\n",
    "    print(\"Emotion Frequencies:\")\n",
    "    for emotion, count in emotion_counter.items():\n",
    "        print(f\"{emotion}: {count}\")\n",
    "\n",
    "\n",
    "\n",
    "    return \"Emotion Frequencies:\", emotion_counter.items()\n",
    "\n",
    "    \n",
    "\n",
    "# creating spark application :\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"FaceDetectionApp\").getOrCreate()\n",
    "\n",
    "# Specify the HDFS folder containing video files\n",
    "# hdfs_video_folder = \"hdfs://your-hdfs-path/to/your/video/folder\"\n",
    "hdfs_video_folder = \"/SmartEdu_src/videos\"\n",
    "\n",
    "# Get a list of all video files in the HDFS folder\n",
    "video_paths = [os.path.join(hdfs_video_folder, file) for file in os.listdir(hdfs_video_folder) if\n",
    "               file.lower().endswith(('.mp4', '.avi', '.mkv'))]\n",
    "\n",
    "# Distribute video paths using Spark\n",
    "videos_rdd = spark.sparkContext.parallelize(video_paths, numSlices=len(video_paths) * 3)\n",
    "\n",
    "# Process videos in parallel using dowork function\n",
    "results_rdd = videos_rdd.map(lambda x : dowork(x))\n",
    "\n",
    "# Collect the results\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n",
    "\n",
    "# Process the collected results as needed\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "test = results[0][1]\n",
    "\n",
    "hello = {}\n",
    "for i in test:\n",
    "    hello[i[0]] = i[1]\n",
    "# hello\n",
    "\n",
    "hash = video_paths[0].split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "\n",
    "result = {\"hash\": hash}\n",
    "\n",
    "emotions = [{\"name\": key, \"count\": value} for key, value in hello.items()]\n",
    "\n",
    "with open(f\"/SmartEdu_src/result/original_frame___{hash}.jpg\", \"rb\") as image_file:\n",
    "    original_frame = base64.b64encode(image_file.read())\n",
    "with open(f\"/SmartEdu_src/result/original_frame_canny___{hash}.jpg\", \"rb\") as image_file:\n",
    "    canny_frame = base64.b64encode(image_file.read())\n",
    "with open(f\"/SmartEdu_src/result/original_frame_denoised___{hash}.jpg\", \"rb\") as image_file:\n",
    "    denoised_frame = base64.b64encode(image_file.read())\n",
    "with open(f\"/SmartEdu_src/result/original_frame_equalized___{hash}.jpg\", \"rb\") as image_file:\n",
    "    equalized_frame = base64.b64encode(image_file.read())\n",
    "with open(f\"/SmartEdu_src/result/output_video___{hash}.mp4\", \"rb\") as image_file:\n",
    "    output_video = base64.b64encode(image_file.read())\n",
    "\n",
    "result[\"processingInfo\"] = {\n",
    "    \"emotions\": emotions,\n",
    "    \"images\": {\n",
    "        \"original\": str(original_frame),\n",
    "        \"canny\": str(canny_frame),\n",
    "        \"denoised\": str(denoised_frame),\n",
    "        \"equalized\": str(equalized_frame)\n",
    "    },\n",
    "    \"processedVideo\": f\"/source/result/output_video___{hash}.mp4\"\n",
    "}\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "json_data = json.dumps(result)\n",
    "\n",
    "# print(json_data)\n",
    "\n",
    "# Set the URL of the API endpoint\n",
    "url = 'http://172.18.0.2:3000/api/videoData'\n",
    "\n",
    "# Set the headers (optional, depending on API requirements)\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "# Send the POST request with JSON payload\n",
    "response = requests.post(url, data=json_data, headers=headers)\n",
    "\n",
    "# Check the response status code\n",
    "if response.status_code == 200:\n",
    "    print(\"Data sent successfully!\")\n",
    "else:\n",
    "    print(\"Failed to send data. Status code:\", response.status_code)\n",
    "    print(\"Response content:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
